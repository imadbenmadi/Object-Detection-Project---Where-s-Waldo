{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Imports\n",
    "Import all required libraries including PyTorch, torchvision, matplotlib, numpy, PIL, and other utilities for web crawling, image processing, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the project\n",
    "\n",
    "# PyTorch and torchvision for deep learning\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Utilities for data handling and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Web crawling and image downloading\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "# Utility for debugging and model summary\n",
    "from torchsummary import summary\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Loading\n",
    "Load character images (like Waldo, Wilma, and Wenda), remove their backgrounds, and prepare them as transparent PNG files. Visualize the processed objects to confirm proper preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for storing original and processed object images\n",
    "object_dir = \"objects/original\"\n",
    "processed_dir = \"objects/processed\"\n",
    "os.makedirs(object_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# URLs for character images\n",
    "character_urls = {\n",
    "    \"waldo\": \"https://static.wikia.nocookie.net/waldo/images/9/9d/Character.Waldo.jpg\",\n",
    "    \"wilma\": \"https://static.wikia.nocookie.net/waldo/images/8/86/Character.Wilma.jpg\",\n",
    "    \"wenda\": \"https://static.wikia.nocookie.net/waldo/images/3/3e/Character.Wenda.jpg\"\n",
    "}\n",
    "\n",
    "# Function to download and process character images\n",
    "def download_and_process_character(character, url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        img_path = os.path.join(object_dir, f\"{character}.jpg\")\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Open the image and convert to RGBA\n",
    "        img = Image.open(img_path).convert(\"RGBA\")\n",
    "        data = np.array(img)\n",
    "\n",
    "        # Remove white/light backgrounds by setting alpha to 0\n",
    "        r, g, b, a = data.T\n",
    "        white_areas = (r > 200) & (g > 200) & (b > 200)\n",
    "        data[..., 3][white_areas.T] = 0\n",
    "\n",
    "        # Save the processed image as a transparent PNG\n",
    "        processed_img = Image.fromarray(data)\n",
    "        processed_path = os.path.join(processed_dir, f\"{character}.png\")\n",
    "        processed_img.save(processed_path)\n",
    "        print(f\"‚úÖ Processed {character} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {character}: {e}\")\n",
    "\n",
    "# Download and process all characters\n",
    "for character, url in character_urls.items():\n",
    "    download_and_process_character(character, url)\n",
    "\n",
    "# Visualize the processed objects\n",
    "def visualize_processed_objects():\n",
    "    processed_images = [\n",
    "        Image.open(os.path.join(processed_dir, f)).convert(\"RGBA\")\n",
    "        for f in os.listdir(processed_dir) if f.endswith(\".png\")\n",
    "    ]\n",
    "    object_names = [os.path.splitext(f)[0] for f in os.listdir(processed_dir) if f.endswith(\".png\")]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, (img, name) in enumerate(zip(processed_images, object_names)):\n",
    "        plt.subplot(1, len(processed_images), i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(name)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the objects\n",
    "visualize_processed_objects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawling for Background Images\n",
    "Use the icrawler library to download background images from Google. Create a function to display sample images from the collection to verify proper downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory for storing background images\n",
    "background_images_dir = \"backgrounds\"\n",
    "os.makedirs(background_images_dir, exist_ok=True)\n",
    "\n",
    "# Function to download background images using GoogleImageCrawler\n",
    "def download_background_images(keywords, max_images_per_keyword=50, output_dir=background_images_dir):\n",
    "    \"\"\"\n",
    "    Downloads background images from Google using specified keywords.\n",
    "\n",
    "    Args:\n",
    "        keywords (list): List of search keywords for background images.\n",
    "        max_images_per_keyword (int): Maximum number of images to download per keyword.\n",
    "        output_dir (str): Directory to save the downloaded images.\n",
    "    \"\"\"\n",
    "    google_crawler = GoogleImageCrawler(storage={\"root_dir\": output_dir})\n",
    "    for keyword in keywords:\n",
    "        google_crawler.crawl(keyword=keyword, max_num=max_images_per_keyword)\n",
    "    print(f\"‚úÖ Downloaded background images for keywords: {keywords}\")\n",
    "\n",
    "# Function to visualize a sample of downloaded background images\n",
    "def visualize_background_images(directory, num_samples=10):\n",
    "    \"\"\"\n",
    "    Visualizes a sample of background images from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing the background images.\n",
    "        num_samples (int): Number of images to display.\n",
    "    \"\"\"\n",
    "    image_files = [\n",
    "        os.path.join(directory, f) for f in os.listdir(directory)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    if not image_files:\n",
    "        print(\"‚ùå No images found in the directory.\")\n",
    "        return\n",
    "\n",
    "    # Randomly select images to display\n",
    "    sampled_images = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, img_path in enumerate(sampled_images):\n",
    "        img = Image.open(img_path)\n",
    "        plt.subplot(1, len(sampled_images), i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image {i + 1}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define keywords for background images\n",
    "background_keywords = [\"doodle background\", \"cluttered background\", \"abstract art background\"]\n",
    "\n",
    "# Download background images\n",
    "download_background_images(background_keywords)\n",
    "\n",
    "# Visualize a sample of the downloaded background images\n",
    "visualize_background_images(background_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Synthetic Dataset\n",
    "Generate a synthetic dataset by placing objects on backgrounds at random positions and scales. Save images with corresponding YOLO-format labels (class_id, x_center, y_center, width, height) for each object placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a synthetic dataset\n",
    "def create_synthetic_dataset(background_dir, object_dir, output_dir, split, img_size=(640, 640), num_images=1000):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset by placing objects on backgrounds.\n",
    "\n",
    "    Args:\n",
    "        background_dir (str): Directory containing background images.\n",
    "        object_dir (str): Directory containing object images with transparency.\n",
    "        output_dir (str): Root directory to save the dataset.\n",
    "        split (str): Dataset split ('train', 'val', or 'test').\n",
    "        img_size (tuple): Size of output images (width, height).\n",
    "        num_images (int): Number of images to generate.\n",
    "    \"\"\"\n",
    "    # Create directories for images and labels\n",
    "    dataset_dir = os.path.join(output_dir, split)\n",
    "    images_dir = os.path.join(dataset_dir, \"images\")\n",
    "    labels_dir = os.path.join(dataset_dir, \"labels\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "    # Load background and object images\n",
    "    background_paths = [\n",
    "        os.path.join(background_dir, f) for f in os.listdir(background_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "    object_paths = [\n",
    "        os.path.join(object_dir, f) for f in os.listdir(object_dir)\n",
    "        if f.lower().endswith(\".png\")\n",
    "    ]\n",
    "    object_names = [os.path.splitext(os.path.basename(f))[0] for f in object_paths]\n",
    "\n",
    "    if not background_paths or not object_paths:\n",
    "        print(\"‚ùå No background or object images found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üèûÔ∏è Found {len(background_paths)} background images and {len(object_paths)} objects.\")\n",
    "    print(f\"üéØ Generating {num_images} synthetic images for {split} set...\")\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Select a random background and object\n",
    "        bg_path = random.choice(background_paths)\n",
    "        obj_path = random.choice(object_paths)\n",
    "        obj_idx = object_paths.index(obj_path)\n",
    "\n",
    "        # Load and resize background\n",
    "        background = Image.open(bg_path).convert(\"RGB\").resize(img_size)\n",
    "\n",
    "        # Load and resize object\n",
    "        obj_image = Image.open(obj_path).convert(\"RGBA\")\n",
    "        scale_factor = random.uniform(0.1, 0.3)  # Object size as a fraction of image size\n",
    "        obj_width = int(img_size[0] * scale_factor)\n",
    "        obj_height = int(obj_width * (obj_image.height / obj_image.width))\n",
    "        obj_image = obj_image.resize((obj_width, obj_height), Image.LANCZOS)\n",
    "\n",
    "        # Randomly position the object on the background\n",
    "        max_x = img_size[0] - obj_width\n",
    "        max_y = img_size[1] - obj_height\n",
    "        x_pos = random.randint(0, max_x)\n",
    "        y_pos = random.randint(0, max_y)\n",
    "\n",
    "        # Paste the object onto the background\n",
    "        background.paste(obj_image, (x_pos, y_pos), obj_image)\n",
    "\n",
    "        # Calculate YOLO-format bounding box\n",
    "        x_center = (x_pos + obj_width / 2) / img_size[0]\n",
    "        y_center = (y_pos + obj_height / 2) / img_size[1]\n",
    "        width = obj_width / img_size[0]\n",
    "        height = obj_height / img_size[1]\n",
    "\n",
    "        # Save the synthetic image\n",
    "        img_filename = f\"{i:05d}.jpg\"\n",
    "        background.save(os.path.join(images_dir, img_filename))\n",
    "\n",
    "        # Save the label in YOLO format\n",
    "        label_filename = f\"{i:05d}.txt\"\n",
    "        with open(os.path.join(labels_dir, label_filename), \"w\") as f:\n",
    "            f.write(f\"{obj_idx} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % 100 == 0 or i == num_images - 1:\n",
    "            print(f\"  Progress: {i + 1}/{num_images} images created.\")\n",
    "\n",
    "    print(f\"‚úÖ Synthetic dataset for {split} set created successfully!\")\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "output_dir = \"synthetic_dataset\"\n",
    "create_synthetic_dataset(background_images_dir, processed_dir, output_dir, \"train\", num_images=5000)\n",
    "create_synthetic_dataset(background_images_dir, processed_dir, output_dir, \"val\", num_images=1000)\n",
    "create_synthetic_dataset(background_images_dir, processed_dir, output_dir, \"test\", num_images=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Loaders\n",
    "Create PyTorch Dataset and DataLoader classes to handle the synthetic dataset. Implement transformations for training, validation, and test sets including normalization and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch Dataset class for object detection\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory of the dataset.\n",
    "            split (str): Dataset split ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # Define paths for images and labels\n",
    "        self.images_dir = os.path.join(root_dir, split, \"images\")\n",
    "        self.labels_dir = os.path.join(root_dir, split, \"labels\")\n",
    "\n",
    "        # Get list of image files\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(self.images_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load corresponding label\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(self.image_files[idx])[0] + \".txt\")\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = f.readline().strip().split()\n",
    "            class_id = int(label_data[0])\n",
    "            bbox = torch.tensor([float(x) for x in label_data[1:]])\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, bbox, class_id\n",
    "\n",
    "# Define transformations for training, validation, and test sets\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset objects for each split\n",
    "dataset_dir = \"synthetic_dataset\"\n",
    "train_dataset = ObjectDetectionDataset(dataset_dir, \"train\", transform=train_transform)\n",
    "val_dataset = ObjectDetectionDataset(dataset_dir, \"val\", transform=val_transform)\n",
    "test_dataset = ObjectDetectionDataset(dataset_dir, \"test\", transform=test_transform)\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Print dataset and DataLoader information\n",
    "print(f\"üìä Dataset Information:\")\n",
    "print(f\"  ‚Ä¢ Training set: {len(train_dataset)} images\")\n",
    "print(f\"  ‚Ä¢ Validation set: {len(val_dataset)} images\")\n",
    "print(f\"  ‚Ä¢ Test set: {len(test_dataset)} images\")\n",
    "\n",
    "print(f\"\\nüîÑ DataLoader Information:\")\n",
    "print(f\"  ‚Ä¢ Training batches: {len(train_loader)}\")\n",
    "print(f\"  ‚Ä¢ Validation batches: {len(val_loader)}\")\n",
    "print(f\"  ‚Ä¢ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Training Data\n",
    "Create functions to visualize batches of training data with bounding boxes and class labels to verify correct dataset creation and loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a batch of training data with bounding boxes and class labels\n",
    "def visualize_training_data(dataloader, class_names, num_samples=8):\n",
    "    \"\"\"\n",
    "    Visualize a batch of training data with bounding boxes and class labels.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        class_names (list): List of class names corresponding to class IDs.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    # Get a batch of data\n",
    "    images, bboxes, class_ids = next(iter(dataloader))\n",
    "\n",
    "    # Limit the number of samples to visualize\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "    # Create a grid for visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_samples):\n",
    "        # Denormalize the image\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        # Extract bounding box and class ID\n",
    "        bbox = bboxes[i].numpy()\n",
    "        class_id = class_ids[i].item()\n",
    "\n",
    "        # Calculate bounding box corners\n",
    "        x_center, y_center, width, height = bbox\n",
    "        x_min = (x_center - width / 2) * img.shape[1]\n",
    "        y_min = (y_center - height / 2) * img.shape[0]\n",
    "        x_max = (x_center + width / 2) * img.shape[1]\n",
    "        y_max = (y_center + height / 2) * img.shape[0]\n",
    "\n",
    "        # Plot the image\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Class: {class_names[class_id]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Draw the bounding box\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                              fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a batch of training data\n",
    "class_names = [\"waldo\", \"wilma\", \"wenda\"]  # Replace with actual class names\n",
    "visualize_training_data(train_loader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Custom Object Detection Model\n",
    "Implement a custom object detection model using a pre-trained ResNet backbone with custom classification and regression heads for object detection. Initialize model weights and output model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom object detection model\n",
    "class CustomObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained=True):\n",
    "        \"\"\"\n",
    "        Initialize the custom object detection model.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of object classes.\n",
    "            pretrained (bool): Whether to use a pre-trained ResNet backbone.\n",
    "        \"\"\"\n",
    "        super(CustomObjectDetectionModel, self).__init__()\n",
    "        \n",
    "        # Use a pre-trained ResNet18 as the backbone\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove the fully connected layers\n",
    "        \n",
    "        # Classification head\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 20 * 20, 256),  # Adjust input size based on feature map dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Regression head\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 20 * 20, 256),  # Adjust input size based on feature map dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 4),  # Output: [x_center, y_center, width, height]\n",
    "            nn.Sigmoid()  # Normalize bounding box coordinates\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for custom layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for custom layers.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Class logits.\n",
    "            torch.Tensor: Bounding box predictions.\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        class_logits = self.classification_head(features)\n",
    "        bbox_preds = self.regression_head(features)\n",
    "        return class_logits, bbox_preds\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = len(class_names)  # Number of object classes\n",
    "model = CustomObjectDetectionModel(num_classes=num_classes, pretrained=True).to(device)\n",
    "\n",
    "# Print model summary\n",
    "input_size = (3, 640, 640)  # Input size: 3 channels (RGB), 640x640 resolution\n",
    "summary(model, input_size=input_size, device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Loss Functions and Optimizer\n",
    "Define appropriate loss functions for classification (CrossEntropyLoss) and bounding box regression (SmoothL1Loss). Create a combined loss function and configure the Adam optimizer with learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "classification_loss_fn = nn.CrossEntropyLoss()  # For class probabilities\n",
    "regression_loss_fn = nn.SmoothL1Loss()  # For bounding box regression\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001  # Learning rate for Adam optimizer\n",
    "weight_decay = 1e-4  # L2 regularization to prevent overfitting\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define a combined loss function\n",
    "def combined_loss(class_pred, bbox_pred, class_target, bbox_target):\n",
    "    \"\"\"\n",
    "    Calculate the combined loss for object detection.\n",
    "\n",
    "    Args:\n",
    "        class_pred (torch.Tensor): Predicted class scores [batch_size, num_classes].\n",
    "        bbox_pred (torch.Tensor): Predicted bounding boxes [batch_size, 4].\n",
    "        class_target (torch.Tensor): Ground truth class indices [batch_size].\n",
    "        bbox_target (torch.Tensor): Ground truth bounding boxes [batch_size, 4].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Total loss.\n",
    "        torch.Tensor: Classification loss.\n",
    "        torch.Tensor: Regression loss.\n",
    "    \"\"\"\n",
    "    # Classification loss\n",
    "    cls_loss = classification_loss_fn(class_pred, class_target)\n",
    "\n",
    "    # Regression loss\n",
    "    reg_loss = regression_loss_fn(bbox_pred, bbox_target)\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = cls_loss + reg_loss\n",
    "    return total_loss, cls_loss, reg_loss\n",
    "\n",
    "# Print the configuration\n",
    "print(\"Loss Functions and Optimizer Configuration:\")\n",
    "print(f\"  ‚Ä¢ Classification Loss: {classification_loss_fn}\")\n",
    "print(f\"  ‚Ä¢ Regression Loss: {regression_loss_fn}\")\n",
    "print(f\"  ‚Ä¢ Optimizer: {optimizer}\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {learning_rate}\")\n",
    "print(f\"  ‚Ä¢ Weight Decay: {weight_decay}\")\n",
    "print(f\"  ‚Ä¢ LR Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Custom Model\n",
    "Implement a comprehensive training loop that handles both training and validation phases. Include early stopping, model checkpoint saving, and learning rate scheduling to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, num_epochs=30, patience=5, device=device):\n",
    "    \"\"\"\n",
    "    Train the custom object detection model with early stopping and learning rate scheduling.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The object detection model.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        loss_fn (function): Combined loss function for classification and regression.\n",
    "        num_epochs (int): Maximum number of epochs.\n",
    "        patience (int): Early stopping patience.\n",
    "        device (torch.device): Device to train on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history containing losses and metrics.\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_cls_loss': [], 'val_cls_loss': [], 'train_reg_loss': [], 'val_reg_loss': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        train_loss, train_cls_loss, train_reg_loss = 0.0, 0.0, 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, bboxes, class_ids in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            images, bboxes, class_ids = images.to(device), bboxes.to(device), class_ids.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            class_pred, bbox_pred = model(images)\n",
    "            loss, cls_loss, reg_loss = loss_fn(class_pred, bbox_pred, class_ids, bboxes)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_cls_loss += cls_loss.item()\n",
    "            train_reg_loss += reg_loss.item()\n",
    "\n",
    "        # Calculate average training losses\n",
    "        train_loss /= len(train_loader)\n",
    "        train_cls_loss /= len(train_loader)\n",
    "        train_reg_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_cls_loss'].append(train_cls_loss)\n",
    "        history['train_reg_loss'].append(train_reg_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, val_cls_loss, val_reg_loss = 0.0, 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, bboxes, class_ids in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                images, bboxes, class_ids = images.to(device), bboxes.to(device), class_ids.to(device)\n",
    "                class_pred, bbox_pred = model(images)\n",
    "                loss, cls_loss, reg_loss = loss_fn(class_pred, bbox_pred, class_ids, bboxes)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_cls_loss += cls_loss.item()\n",
    "                val_reg_loss += reg_loss.item()\n",
    "\n",
    "        # Calculate average validation losses\n",
    "        val_loss /= len(val_loader)\n",
    "        val_cls_loss /= len(val_loader)\n",
    "        val_reg_loss /= len(val_loader)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_cls_loss'].append(val_cls_loss)\n",
    "        history['val_reg_loss'].append(val_reg_loss)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Train CLS Loss: {train_cls_loss:.4f} | Val CLS Loss: {val_cls_loss:.4f}\")\n",
    "        print(f\"Train REG Loss: {train_reg_loss:.4f} | Val REG Loss: {val_reg_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"‚úÖ Model improved. Saved best model.\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"‚ö†Ô∏è No improvement. Early stopping counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=combined_loss,\n",
    "    num_epochs=30,\n",
    "    patience=5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot total loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history['val_loss'], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Total Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot classification and regression losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_cls_loss'], label=\"Train CLS Loss\")\n",
    "    plt.plot(epochs, history['val_cls_loss'], label=\"Val CLS Loss\")\n",
    "    plt.plot(epochs, history['train_reg_loss'], label=\"Train REG Loss\")\n",
    "    plt.plot(epochs, history['val_reg_loss'], label=\"Val REG Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Classification and Regression Losses\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Training Metrics\n",
    "Plot training and validation losses over time including total loss, classification loss, and regression loss. Visualize metrics to analyze model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize training and validation metrics\n",
    "def plot_training_metrics(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation losses over epochs.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dictionary containing training and validation losses.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Create a figure for the plots\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot total loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label=\"Training Loss\", color=\"blue\", marker=\"o\")\n",
    "    plt.plot(epochs, history['val_loss'], label=\"Validation Loss\", color=\"orange\", marker=\"o\")\n",
    "    plt.title(\"Total Loss Over Epochs\", fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot classification and regression losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_cls_loss'], label=\"Train Classification Loss\", color=\"green\", marker=\"o\")\n",
    "    plt.plot(epochs, history['val_cls_loss'], label=\"Val Classification Loss\", color=\"red\", marker=\"o\")\n",
    "    plt.plot(epochs, history['train_reg_loss'], label=\"Train Regression Loss\", color=\"purple\", marker=\"o\")\n",
    "    plt.plot(epochs, history['val_reg_loss'], label=\"Val Regression Loss\", color=\"brown\", marker=\"o\")\n",
    "    plt.title(\"Classification and Regression Losses\", fontsize=14)\n",
    "    plt.xlabel(\"Epochs\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to visualize training metrics\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Inference on Test Data\n",
    "Create an inference pipeline to run predictions on test data. Implement non-maximum suppression if needed and calculate performance metrics like precision, recall, and IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms\n",
    "\n",
    "def run_inference(model, dataloader, iou_threshold=0.5, device=device):\n",
    "    \"\"\"\n",
    "    Run inference on the test dataset and calculate performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained object detection model.\n",
    "        dataloader (DataLoader): DataLoader for the test dataset.\n",
    "        iou_threshold (float): IoU threshold for non-maximum suppression.\n",
    "        device (torch.device): Device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing precision, recall, IoU, and predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, bboxes, class_ids in tqdm(dataloader, desc=\"Running Inference\"):\n",
    "            images = images.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "            class_ids = class_ids.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            class_logits, bbox_preds = model(images)\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            class_probs = torch.softmax(class_logits, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                # Extract predictions for the current image\n",
    "                probs = class_probs[i]\n",
    "                preds = bbox_preds[i]\n",
    "\n",
    "                # Apply non-maximum suppression\n",
    "                scores, labels = probs.max(dim=0)\n",
    "                keep = nms(preds, scores, iou_threshold)\n",
    "\n",
    "                # Store predictions and ground truths\n",
    "                all_predictions.append({\n",
    "                    \"boxes\": preds[keep].cpu(),\n",
    "                    \"scores\": scores[keep].cpu(),\n",
    "                    \"labels\": labels[keep].cpu()\n",
    "                })\n",
    "                all_ground_truths.append({\n",
    "                    \"boxes\": bboxes[i].cpu(),\n",
    "                    \"labels\": class_ids[i].cpu()\n",
    "                })\n",
    "\n",
    "    return {\"predictions\": all_predictions, \"ground_truths\": all_ground_truths}\n",
    "\n",
    "def calculate_metrics(predictions, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and IoU for the predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): List of predicted bounding boxes and labels.\n",
    "        ground_truths (list): List of ground truth bounding boxes and labels.\n",
    "        iou_threshold (float): IoU threshold for matching predictions to ground truths.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing precision, recall, and IoU.\n",
    "    \"\"\"\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    iou_scores = []\n",
    "\n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        pred_boxes = pred[\"boxes\"]\n",
    "        pred_labels = pred[\"labels\"]\n",
    "        gt_boxes = gt[\"boxes\"]\n",
    "        gt_labels = gt[\"labels\"]\n",
    "\n",
    "        for i, pred_box in enumerate(pred_boxes):\n",
    "            ious = torchvision.ops.box_iou(pred_box.unsqueeze(0), gt_boxes)\n",
    "            max_iou, max_idx = ious.max(dim=1)\n",
    "\n",
    "            if max_iou > iou_threshold and pred_labels[i] == gt_labels[max_idx]:\n",
    "                tp += 1\n",
    "                iou_scores.append(max_iou.item())\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        fn += len(gt_boxes) - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    mean_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"mean_iou\": mean_iou}\n",
    "\n",
    "# Run inference on the test dataset\n",
    "inference_results = run_inference(model, test_loader)\n",
    "\n",
    "# Calculate performance metrics\n",
    "metrics = calculate_metrics(inference_results[\"predictions\"], inference_results[\"ground_truths\"])\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a YOLO Model\n",
    "Load a pre-trained YOLOv5 model and fine-tune it on the same synthetic dataset. Configure the model for the specific detection task with the appropriate number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install YOLOv5 dependencies\n",
    "!pip install -q ultralytics\n",
    "\n",
    "# Import YOLOv5 from ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pre-trained YOLOv5 model\n",
    "yolo_model = YOLO('yolov5s.pt')  # Use the small YOLOv5 model for fine-tuning\n",
    "\n",
    "# Update the model configuration for the custom dataset\n",
    "yolo_model.model.nc = len(class_names)  # Set the number of classes\n",
    "yolo_model.model.names = class_names  # Set the class names\n",
    "\n",
    "# Define paths for training, validation, and test datasets\n",
    "train_data_path = os.path.join(dataset_dir, \"train\")\n",
    "val_data_path = os.path.join(dataset_dir, \"val\")\n",
    "test_data_path = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Fine-tune the YOLOv5 model on the custom dataset\n",
    "yolo_model.train(\n",
    "    data={\n",
    "        'train': train_data_path,\n",
    "        'val': val_data_path,\n",
    "        'names': class_names\n",
    "    },\n",
    "    epochs=30,  # Number of epochs for fine-tuning\n",
    "    batch=16,  # Batch size\n",
    "    imgsz=640,  # Image size\n",
    "    device=device,  # Use GPU if available\n",
    "    project='yolo_finetune',  # Directory to save results\n",
    "    name='custom_yolo'  # Name of the experiment\n",
    ")\n",
    "\n",
    "# Evaluate the fine-tuned YOLOv5 model on the test dataset\n",
    "results = yolo_model.val(data=test_data_path, imgsz=640, batch=16, device=device)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Precision: {results['metrics']['precision']:.4f}\")\n",
    "print(f\"Recall: {results['metrics']['recall']:.4f}\")\n",
    "print(f\"mAP@0.5: {results['metrics']['map50']:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {results['metrics']['map']:.4f}\")\n",
    "\n",
    "# Visualize predictions on a batch of test images\n",
    "test_images = [os.path.join(test_data_path, \"images\", f) for f in os.listdir(os.path.join(test_data_path, \"images\"))]\n",
    "predictions = yolo_model.predict(source=test_images[:8], imgsz=640, conf=0.25, device=device)\n",
    "\n",
    "# Display predictions\n",
    "for pred in predictions:\n",
    "    pred.plot()  # Visualize predictions with bounding boxes and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Models\n",
    "Compare the performance of both models (custom and YOLO) using standard metrics like mAP, precision, recall, and F1-score. Visualize detection results from both models on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, iou_threshold=0.5, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the object detection model using standard metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained object detection model.\n",
    "        dataloader (DataLoader): DataLoader for the test dataset.\n",
    "        class_names (list): List of class names.\n",
    "        iou_threshold (float): IoU threshold for matching predictions to ground truths.\n",
    "        device (torch.device): Device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing precision, recall, F1-score, mAP, and IoU metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    iou_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, bboxes, class_ids in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "            class_ids = class_ids.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            class_logits, bbox_preds = model(images)\n",
    "            class_probs = torch.softmax(class_logits, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                # Extract predictions for the current image\n",
    "                probs = class_probs[i]\n",
    "                preds = bbox_preds[i]\n",
    "\n",
    "                # Match predictions to ground truths using IoU\n",
    "                ious = box_iou(preds, bboxes[i].unsqueeze(0))\n",
    "                matched = ious > iou_threshold\n",
    "\n",
    "                # Store predictions and ground truths\n",
    "                all_preds.append((probs.argmax(dim=0).item(), preds))\n",
    "                all_targets.append((class_ids[i].item(), bboxes[i]))\n",
    "\n",
    "                # Calculate IoU for matched boxes\n",
    "                if matched.any():\n",
    "                    iou_scores.append(ious[matched].mean().item())\n",
    "\n",
    "    # Calculate precision, recall, F1-score, and mAP\n",
    "    y_true = [target[0] for target in all_targets]\n",
    "    y_pred = [pred[0] for pred in all_preds]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    mean_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n",
    "    map_score = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"mAP\": map_score\n",
    "    }\n",
    "\n",
    "# Evaluate the custom model\n",
    "custom_model_metrics = evaluate_model(model, test_loader, class_names)\n",
    "\n",
    "# Print evaluation metrics for the custom model\n",
    "print(\"Custom Model Evaluation:\")\n",
    "print(f\"Precision: {custom_model_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {custom_model_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {custom_model_metrics['f1_score']:.4f}\")\n",
    "print(f\"Mean IoU: {custom_model_metrics['mean_iou']:.4f}\")\n",
    "print(f\"mAP: {custom_model_metrics['mAP']:.4f}\")\n",
    "\n",
    "# Evaluate the YOLO model\n",
    "yolo_predictions = yolo_model.predict(source=os.path.join(dataset_dir, \"test\", \"images\"), imgsz=640, conf=0.25, device=device)\n",
    "yolo_metrics = yolo_model.val(data=os.path.join(dataset_dir, \"test\"), imgsz=640, batch=16, device=device)\n",
    "\n",
    "# Print evaluation metrics for the YOLO model\n",
    "print(\"\\nYOLO Model Evaluation:\")\n",
    "print(f\"Precision: {yolo_metrics['metrics']['precision']:.4f}\")\n",
    "print(f\"Recall: {yolo_metrics['metrics']['recall']:.4f}\")\n",
    "print(f\"mAP@0.5: {yolo_metrics['metrics']['map50']:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {yolo_metrics['metrics']['map']:.4f}\")\n",
    "\n",
    "# Visualize predictions for both models\n",
    "def visualize_predictions(model, dataloader, class_names, num_images=4, device=device):\n",
    "    \"\"\"\n",
    "    Visualize predictions from the model on test images.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained object detection model.\n",
    "        dataloader (DataLoader): DataLoader for the test dataset.\n",
    "        class_names (list): List of class names.\n",
    "        num_images (int): Number of images to visualize.\n",
    "        device (torch.device): Device to run inference on.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images, bboxes, class_ids = next(iter(dataloader))\n",
    "    images = images[:num_images].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        class_logits, bbox_preds = model(images)\n",
    "        class_probs = torch.softmax(class_logits, dim=1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        pred_bbox = bbox_preds[i].cpu().numpy()\n",
    "        pred_class = class_probs[i].argmax().item()\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Pred: {class_names[pred_class]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        x_center, y_center, width, height = pred_bbox\n",
    "        x_min = (x_center - width / 2) * img.shape[1]\n",
    "        y_min = (y_center - height / 2) * img.shape[0]\n",
    "        x_max = (x_center + width / 2) * img.shape[1]\n",
    "        y_max = (y_center + height / 2) * img.shape[0]\n",
    "\n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, edgecolor=\"red\", fill=False, linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions for the custom model\n",
    "print(\"\\nCustom Model Predictions:\")\n",
    "visualize_predictions(model, test_loader, class_names)\n",
    "\n",
    "# Visualize predictions for the YOLO model\n",
    "print(\"\\nYOLO Model Predictions:\")\n",
    "yolo_predictions = yolo_model.predict(source=os.path.join(dataset_dir, \"test\", \"images\"), imgsz=640, conf=0.25, device=device)\n",
    "for pred in yolo_predictions[:4]:\n",
    "    pred.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
