{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Project - Where's Waldo?\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project implements a simplified object detection system to identify and locate cartoon characters from the \"Where's Waldo?\" series. The system uses both a custom-built neural network architecture and a fine-tuned YOLOv8 model, comparing their performance on a synthetic dataset.\n",
    "\n",
    "## Dataset Creation\n",
    "\n",
    "### Character Selection\n",
    "We selected three main characters from the \"Where's Waldo?\" series:\n",
    "- **Waldo**: Characterized by his iconic red and white striped shirt and hat\n",
    "- **Wilma**: Waldo's friend with blue clothing\n",
    "- **Wenda**: Character with pink/red striped clothing\n",
    "\n",
    "### Background Collection\n",
    "- Used `icrawler` library to download 100+ themed background images\n",
    "- Backgrounds include cartoon scenes, crowded illustrations, and Where's Waldo-style puzzles\n",
    "- Images were resized to 640×640 pixels for consistency\n",
    "\n",
    "### Synthetic Dataset Generation Process\n",
    "1. **Object Preparation**:\n",
    "   - Characters were extracted with transparent backgrounds using image editing tools\n",
    "   - Each character was saved as a PNG with alpha channel\n",
    "\n",
    "2. **Dataset Generation**:\n",
    "   - Generated 5,000 training images, 1,000 validation images, and 200 test images\n",
    "   - For each image:\n",
    "     - Randomly selected a background\n",
    "     - Randomly chose one character\n",
    "     - Randomly scaled the character (50-100% of original size)\n",
    "     - Placed character at random coordinates on the background\n",
    "     - Calculated bounding box coordinates in YOLO format: `<class_id> <x_center> <y_center> <width> <height>`\n",
    "     - Saved the image and corresponding annotation\n",
    "\n",
    "3. **Data Format**:\n",
    "   - Images saved as JPG files\n",
    "   - Annotations saved in YOLO format (normalized coordinates)\n",
    "   - Directory structure compatible with both custom training and YOLOv8\n",
    "\n",
    "## Custom Model Architecture\n",
    "\n",
    "### Backbone Network\n",
    "- **ResNet18**: Pre-trained on ImageNet, used as feature extractor\n",
    "- Removed final fully connected layer to obtain feature maps\n",
    "- Extracted features from multiple layers for multi-scale detection\n",
    "\n",
    "### Feature Pyramid Network (FPN)\n",
    "- Implemented a simplified FPN to merge features from different scales\n",
    "- Used 1×1 convolutions to reduce channel dimensions\n",
    "- Added skip connections between layers for better gradient flow\n",
    "\n",
    "### Detection Heads\n",
    "1. **Classification Head**:\n",
    "   - Two fully connected layers (512→256→3)\n",
    "   - Outputs class probabilities for the 3 characters\n",
    "   - Softmax activation for final prediction\n",
    "\n",
    "2. **Regression Head**:\n",
    "   - Two fully connected layers (512→256→4)\n",
    "   - Outputs normalized coordinates (x_center, y_center, width, height)\n",
    "   - No activation function on output layer\n",
    "\n",
    "### Model Summary\n",
    "```\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Conv2d-1         [1, 64, 320, 320]           9,408\n",
    "       BatchNorm2d-2         [1, 64, 320, 320]             128\n",
    "              ReLU-3         [1, 64, 320, 320]               0\n",
    "         MaxPool2d-4         [1, 64, 160, 160]               0\n",
    "        BasicBlock-5         [1, 64, 160, 160]          73,984\n",
    "        BasicBlock-6         [1, 64, 160, 160]          73,984\n",
    "        BasicBlock-7        [1, 128, 80, 80]          230,144\n",
    "        BasicBlock-8        [1, 128, 80, 80]          295,424\n",
    "        BasicBlock-9        [1, 256, 40, 40]          919,296\n",
    "       BasicBlock-10        [1, 256, 40, 40]        1,180,672\n",
    "       BasicBlock-11        [1, 512, 20, 20]        3,675,648\n",
    "       BasicBlock-12        [1, 512, 20, 20]        4,723,712\n",
    "         AdaptiveAvgPool2d-13            [1, 512, 1, 1]               0\n",
    "                  FPN-14            [1, 256, 20, 20]          45,056\n",
    "           SPP-Module-15                   [1, 256]         393,472\n",
    "             Linear-16                   [1, 256]         131,328\n",
    "             Linear-17                     [1, 3]             771\n",
    "             Linear-18                   [1, 256]         131,328\n",
    "             Linear-19                     [1, 4]           1,028\n",
    "================================================================\n",
    "Total params: 11,885,383\n",
    "Trainable params: 11,885,383\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "## Training Process\n",
    "\n",
    "### Data Augmentation\n",
    "- **Horizontal flips**: 50% probability\n",
    "- **Color jitter**: Randomized brightness (±0.2), contrast (±0.2), saturation (±0.2), hue (±0.1)\n",
    "- **Normalization**: Using ImageNet mean [0.485, 0.456, 0.406] and std [0.229, 0.224, 0.225]\n",
    "\n",
    "### Loss Functions\n",
    "- **Classification Loss**: Cross-Entropy Loss\n",
    "- **Regression Loss**: Smooth L1 Loss (Huber Loss)\n",
    "- **Total Loss**: Classification Loss + λ * Regression Loss (λ=10)\n",
    "\n",
    "### Training Parameters\n",
    "- **Optimizer**: Adam with learning rate of 0.001\n",
    "- **Weight decay**: 1e-4 for regularization\n",
    "- **Batch size**: 32\n",
    "- **Epochs**: 50 with early stopping\n",
    "- **Learning rate scheduler**: ReduceLROnPlateau (patience=5, factor=0.1)\n",
    "- **Device**: CUDA GPU (when available)\n",
    "\n",
    "### Training Loop\n",
    "The training loop included:\n",
    "1. Forward pass through the model\n",
    "2. Computation of classification and regression losses\n",
    "3. Backpropagation of gradients\n",
    "4. Optimizer step\n",
    "5. Learning rate scheduling\n",
    "6. Validation after each epoch\n",
    "7. Model checkpoint saving (best model based on validation loss)\n",
    "8. Early stopping if no improvement for 10 epochs\n",
    "\n",
    "## YOLOv8 Implementation\n",
    "\n",
    "### Model Selection\n",
    "- Used the YOLOv8 nano model (`yolov8n.pt`)\n",
    "- Lightweight model with 3.2M parameters for efficient training and inference\n",
    "\n",
    "### Fine-tuning Process\n",
    "- Created YAML configuration file with dataset paths and class names\n",
    "- Used Ultralytics API for fine-tuning:\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.train(data='dataset.yaml', epochs=50, imgsz=640, batch=16)\n",
    "```\n",
    "\n",
    "### YOLOv8 Training Parameters\n",
    "- **Learning rate**: 0.01 with cosine scheduler\n",
    "- **Optimizer**: SGD with momentum\n",
    "- **Image size**: 640×640\n",
    "- **Batch size**: 16\n",
    "- **Epochs**: 50\n",
    "- **Augmentation**: Default YOLOv8 augmentations (mosaic, mixup, etc.)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Performance Metrics\n",
    "Both models were evaluated using:\n",
    "1. **Mean Average Precision (mAP@0.5)**: Primary metric for object detection\n",
    "2. **Intersection over Union (IoU)**: Measures overlap between predicted and ground truth boxes\n",
    "3. **Classification Accuracy**: Percentage of correctly classified objects\n",
    "4. **Precision**: TP / (TP + FP)\n",
    "5. **Recall**: TP / (TP + FN)\n",
    "6. **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### Results Visualization\n",
    "- Loss curves (training and validation)\n",
    "- Prediction visualization on test images\n",
    "- Confusion matrices for classification accuracy\n",
    "- Precision-Recall curves\n",
    "- mAP calculation at different IoU thresholds\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Key Libraries Used\n",
    "- **PyTorch**: Main deep learning framework\n",
    "- **torchvision**: For pre-trained models and transforms\n",
    "- **NumPy**: For numerical operations\n",
    "- **Matplotlib/Seaborn**: For visualization\n",
    "- **Pillow (PIL)**: Image processing\n",
    "- **tqdm**: Progress bars\n",
    "- **Ultralytics**: YOLO implementation\n",
    "- **icrawler**: Web scraping for background images\n",
    "\n",
    "### Data Loading\n",
    "- Custom `SyntheticDataset` class inheriting from `torch.utils.data.Dataset`\n",
    "- Implemented `__getitem__` and `__len__` methods\n",
    "- Used `DataLoader` with shuffling, batching, and multi-processing\n",
    "\n",
    "### Inference Pipeline\n",
    "1. Load image and preprocess (resize, normalize)\n",
    "2. Forward pass through model\n",
    "3. Extract classification probabilities and bounding box coordinates\n",
    "4. Apply confidence threshold (0.5)\n",
    "5. Convert normalized coordinates to pixel coordinates\n",
    "6. Draw bounding boxes and class labels on image\n",
    "\n",
    "### Early Stopping Implementation\n",
    "```python\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        torch.save(model.state_dict(), path)\n",
    "```\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "1. **Challenge**: Bounding box regression accuracy\n",
    "   - **Solution**: Implemented Smooth L1 Loss and increased its weight in total loss\n",
    "\n",
    "2. **Challenge**: Model overfitting due to simple backgrounds\n",
    "   - **Solution**: Added more complex backgrounds and increased data augmentation\n",
    "\n",
    "3. **Challenge**: Balancing classification and regression tasks\n",
    "   - **Solution**: Tuned the λ parameter to balance the two loss components\n",
    "\n",
    "4. **Challenge**: Small objects detection\n",
    "   - **Solution**: Implemented Feature Pyramid Network to enhance multi-scale capabilities\n",
    "\n",
    "## How to Use the Project\n",
    "\n",
    "1. **Dataset Generation**:\n",
    "   ```python\n",
    "   python generate_dataset.py --num_train 5000 --num_val 1000 --num_test 200\n",
    "   ```\n",
    "\n",
    "2. **Custom Model Training**:\n",
    "   ```python\n",
    "   python train.py --epochs 50 --batch_size 32 --lr 0.001\n",
    "   ```\n",
    "\n",
    "3. **YOLOv8 Training**:\n",
    "   ```python\n",
    "   python train_yolo.py --epochs 50 --batch_size 16\n",
    "   ```\n",
    "\n",
    "4. **Inference**:\n",
    "   ```python\n",
    "   python detect.py --model_path best_model.pth --image_path test.jpg\n",
    "   ```\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "1. Extend to multi-object detection with Non-Maximum Suppression\n",
    "2. Implement more advanced backbones (EfficientNet, Vision Transformers)\n",
    "3. Add more background diversity for better generalization\n",
    "4. Test on real Where's Waldo puzzle images\n",
    "5. Implement anchor-based detection for better accuracy\n",
    "6. Add attention mechanisms to focus on important features\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrates the implementation of a simplified object detection system using both custom neural networks and state-of-the-art YOLOv8 models. The synthetic dataset approach provides a controlled environment for training and evaluation, while the comparison between models offers insights into different object detection paradigms.\n",
    "\n",
    "Similar code found with 3 license types"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
